{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "broad-balance",
   "metadata": {},
   "source": [
    "## The Pessimistic Principle\n",
    "\n",
    "In this assignment, our goal is to implement and evaluate the performance of the lower confidence bound algorithm for the four Bernoulli arms example covered in Lecture 1.\n",
    "\n",
    "![Four arms](./graphs/fourarms.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9802960d",
   "metadata": {},
   "source": [
    "We will run 200 trials. For each trial, generate an offline dataset which consists of 500 rewards under the first arm, 500 rewards under the second arm, 1 reward under the third arm and 500 rewards under the last arm. We then apply the LCB algorithm with the constant $c$ equal to $0, 0.1, 1$. For each choice of $c$, compute the regret (see the definition on Page 20 of Lecture 10) in each trial. Then aggregate the regret over 200 trials, for each $c$. Print and compare these three regrets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836dab71",
   "metadata": {},
   "source": [
    "#### First, we implement the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "finite-approach",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import rc\n",
    "\n",
    "# you might see an error that the module \"tkinter\" is not installed. If on Mac Os you can install it through the terminal via \"brew install python-tk@3.9\". General help can as always be found on stackoverflow: \"https://stackoverflow.com/questions/25905540/importerror-no-module-named-tkinter\" \n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "bandit_probabilities = [0.10, 0.40, 0.10, 0.10]\n",
    "\n",
    "number_of_bandits = len(bandit_probabilities) # = n_actions\n",
    "    \n",
    "action_space = np.arange(number_of_bandits) # =[0,1,2,3]\n",
    "\n",
    "number_of_trials = 200\n",
    "\n",
    "arms = np.zeros(timesteps, dtype=int)\n",
    "\n",
    "def step(action):\n",
    "    rand = np.random.random()  # [0.0,1.0)\n",
    "    reward = 1.0 if (rand < bandit_probabilities[action]) else 0.0\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e068d27",
   "metadata": {},
   "source": [
    "#### Second, we review the lower confidence bound algorithm and implement it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610f6720",
   "metadata": {},
   "source": [
    "<img src=\"graphs/LCB.png\" width=700>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "patient-heater",
   "metadata": {},
   "outputs": [],
   "source": [
    "lcb_constants = [0, 0.1, 1.0]\n",
    "sample_size = [500, 500, 1, 500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "historic-entrance",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.027 0.    0.   ]\n"
     ]
    }
   ],
   "source": [
    "def lower_confidence_bound_policy(c, actions, q_values, num_invocations):\n",
    "    lower_confidence_bounds = [q_values[action] - c * np.sqrt(np.log(sum(num_invocations)) / (num_invocations[action])) if num_invocations[action] > 0 else np.inf for action in actions]\n",
    "    return np.random.choice([action_ for action_, value_ in enumerate(lower_confidence_bounds) if value_ == np.max(lower_confidence_bounds)])\n",
    "\n",
    "\n",
    "regret = np.zeros((len(lcb_constants), number_of_trials), dtype=float)\n",
    "\n",
    "for lcb_constant_counter, lcb_constant in enumerate(lcb_constants):\n",
    "    for trial in range(number_of_trials):\n",
    "        n = np.zeros(number_of_bandits, dtype=int)\n",
    "        q = np.zeros(number_of_bandits, dtype=float)\n",
    "\n",
    "        for j in range(len(sample_size)):\n",
    "            for t in range(sample_size[j]):\n",
    "                action = j\n",
    "                r = step(action)\n",
    "\n",
    "                # updating action counter and expected reward Q\n",
    "                n[action] += 1\n",
    "                q[action] = q[action] + 1.0 / (n[action] + 1) * (r - q[action])\n",
    "\n",
    "        regret[lcb_constant_counter, trial] = np.max(bandit_probabilities) - bandit_probabilities[lower_confidence_bound_policy(lcb_constant, action_space, q, sample_size)]\n",
    "\n",
    "print(np.mean(regret, axis=1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
