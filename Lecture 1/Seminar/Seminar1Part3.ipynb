{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "accurate-distributor",
   "metadata": {},
   "source": [
    "# Part 3: Proof of the Existence of the Optimal Stationary Policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9565dd81",
   "metadata": {},
   "source": [
    "## Revisting different classes of policies\n",
    "<img src=\"graphs/Policy.png\" width=900 />\n",
    "<img src=\"graphs/Policies.png\" width=400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-assets",
   "metadata": {},
   "source": [
    "## Sketch of the Proof\n",
    "\n",
    "$\\newcommand{\\P}{\\mathbb{P}}$\n",
    "$\\newcommand{\\E}{\\mathbb{E}}$\n",
    "$\\newcommand{\\S}{\\mathcal{S}}$\n",
    "$\\newcommand{\\A}{\\mathcal{A}}$\n",
    "$\\newcommand{\\R}{\\mathbb{R}}$\n",
    "$\\newcommand{\\L}{\\mathcal{L}}$\n",
    "$\\newcommand{\\Vpi}{V^{\\pi}}$\n",
    "\n",
    "We aim to show under the MDP assumption (Markovianity & time-homogeneity), there exists an optimal stationary deterministic policy whose value is no worse than any history-dependent policy. Formally speaking, we aim to show the existence of $\\pi^*\\in$ SD such that\n",
    "\n",
    "$$V^{\\pi^*}(s)=\\sup_{\\pi \\in \\mathrm{HR}} \\Vpi(s)$$\n",
    "\n",
    "for any state $s$, where HR is the class of history-dependent (random) policies and SD is the class of stationary deterministic policies. We also use MR to denote the class of Markov (random) policies. \n",
    "\n",
    "Here the function $\\Vpi(s)$ is the value function, that is, the expected discounted reward in an infinite horizon with policy $\\pi$ and initial state $s$:\n",
    "$$\n",
    "\\Vpi(s) = \\sum_{t = 0}^{\\infty} \\gamma^t E^{\\pi}(R_t \\mid S_0 = s)$$\n",
    "\n",
    "To simplify the proof, we focus on the setting where $R_t$ is a deterministic function of $(A_t,S_t)$, i.e., $R_t=r(S_t, A_t)$ for some reward function $r$. \n",
    "\n",
    "Recall that the *Markov Assumption* meant *memorylessness* in the sense that for any state $s$\n",
    "\n",
    "$$\\P(S_{t+1} = s \\mid A_t, S_t, \\ldots, A_1,S_1,A_0,S_0) = \\P(S_{t+1} = s \\mid A_t, S_t)$$\n",
    "\n",
    "In addition, neither $r$ nor $\\P(S_{t+1} = s \\mid A_t, S_t)$ depend on $t$, under the *time-homogeneity* assumption. \n",
    "\n",
    "We will prove this statement in three steps. We will follow the proof of Shi et al (2020).\n",
    "\n",
    "1. First, we show that Markov Policies are as good as history-dependent policies: $\\sup_{\\pi \\in \\mathrm{HR}} \\Vpi(s) = \\sup_{\\pi \\in \\mathrm{MR}} \\Vpi(s)$ for any $s$.\n",
    "\n",
    "2. For any function $\\nu$ that satisfies the Bellman optimality equation we have $\\nu(s) \\ge \\sup_{\\pi \\in \\mathrm{MR}} \\Vpi(s)$. \n",
    "\n",
    "3. Existence of $\\pi^* \\in \\mathrm{SD}$ such that $V^{\\pi^*}$ satisfies the Bellman optimality equation. \n",
    "\n",
    "Combining 2. together with 3. yields $V^{\\pi^*}(s) \\ge \\sup_{\\pi \\in \\mathrm{MR}} \\Vpi(s)$. Together with 1., we obtain $V^{\\pi^*}(s) \\ge \\sup_{\\pi \\in \\mathrm{HR}} \\Vpi(s)$. The proof is completed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "developing-nurse",
   "metadata": {},
   "source": [
    "### Proof of 1.\n",
    "\n",
    "We prove the following equation. For any policy $\\pi \\in \\mathrm{HR}$, there exists a policy $\\bar \\pi \\in \\mathrm{MR}$ such that for all start states $s_0$ and any state $s$ and action $a$ we have \n",
    "\n",
    "$$ \\P^\\pi(A_t = a, S_t = s | S_0 = s_0) = \\P^{\\bar \\pi}(A_t = a, S_t = s | S_0 = s_0) $$\n",
    "\n",
    "by induction. Here, $\\bar \\pi$ is allowed to depend on $s_0$. But it does not depend on $s$ and $a$. This equation implies that for any $s_0$ and $\\pi$, there exists a Markov policy $\\bar \\pi$ under which the generated state-action pair at each time has the same conditional distribution as that under $\\pi$ given $S_0=s_0$. Since the reward is a deterministic function of the state-action pair, the expected values of rewards given $S_0=s_0$ under $\\pi$ and $\\bar \\pi$ are the same. The proof is thus completed. \n",
    "\n",
    "For $t = 0$ the statement is trivial. Now assume that the statement holds for some time $t$. We will prove that then it also holds for $t + 1$. \n",
    "\n",
    "By conditional probability we have for all $s$ that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\P^\\pi(S_{t+1} = s | S_0 = s_0) &= \\E^\\pi(\\P(S_{t+1} = s | A_t, S_t, S_0 = s_0) | S_0 = s_0) \\\\\n",
    "&= \\E^\\pi(\\P(S_{t+1} = s | A_t, S_t) | S_0 = s_0) \\qquad \\qquad \\text{by Markov Assumption} \\\\\n",
    "&= \\E^{\\bar \\pi}(\\P(S_{t+1} = s | A_t, S_t) | S_0 = s_0) \\qquad \\qquad \\text{by induction hypothesis}  \\\\\n",
    "&= \\P^{\\bar \\pi}(S_{t+1} = s | S_0 = s_0).\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Hence we have that\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\P^{\\pi}(A_{t+1} = a, S_{t+1} = s \\mid S_0 = s_0) &= \\P^\\pi(A_{t+1} = a | S_{t+1} = s, S_0 = s_0) \\P^{\\pi}(S_{t+1} = s | S_0 = s_0) \\\\\n",
    "&= \\P^{\\bar \\pi}(A_{t+1} = a | S_{t+1} = s, S_0 = s_0) \\P^{\\bar \\pi}(S_{t+1} = s | S_0 = s_0) \\\\\n",
    "&= \\P^{\\bar \\pi}(A_{t+1} = a, S_{t+1} = s \\mid s_0),\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "where the action probability of $\\bar \\pi$ at time step $t +1$ is defined to be equal to the action probability of $\\pi$ given $S_0=s_0$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "transsexual-treasury",
   "metadata": {},
   "source": [
    "### Proof of 2.\n",
    "Recall that a function $\\nu: s \\to \\mathbb{R}$ fulfills the Bellman optimality equations if  \n",
    "$$\n",
    "\\nu(s) = \\max_{a} \\left[r(s, a) + \\gamma \\sum_{s'} \\nu(s') \\P(S_{t+1}=s'|A_t=a,S_t=s)\\right]\n",
    "$$\n",
    "\n",
    "<!--For each state $s$, there exists an action that maximizes the right-hand-side. This allows us to define a stationary deterministic policy that takes $s$ as input and outputs the argmax. The Bellman optimality equation can be rewritten as \n",
    "\n",
    "$$\n",
    "\\nu(s) = \\sup_{\\pi \\in \\mathrm{SD}} \\left[r(s, \\pi(s)) + \\gamma \\sum_{s'} \\nu(s') \\P(S_{t+1}=s'|A_t= \\pi(s),S_t=s)\\right]\n",
    "$$-->\n",
    "\n",
    "I want to stress the point that at this point we have not proven that such a $\\nu$ exists. Nonetheless we can make statements that hold for all $\\nu$ that fulfill these equations. We now prove Step 2, that is $\\nu(s) \\ge \\sup_{\\pi \\in \\mathrm{MR}} \\Vpi(s)$.\n",
    "<!--We first prove $\\ge$ and then $\\le$.\n",
    "\n",
    "#### Proof of $\\ge$-->\n",
    "\n",
    "Let $\\pi=\\{\\pi_k\\}_k \\in \\mathrm{MR}$ be arbitrary where $\\pi_k$ denotes the mapping the agent implements at time $k$. We aim to show $\\nu(s)\\ge \\Vpi(s)$. \n",
    "As $\\nu$ fulfills the Bellman equations we in particular have that \n",
    "$$\n",
    "\\nu(s) \\ge r(s, a) + \\gamma \\sum_{s'} \\nu(s') \\P(S_{t+1}=s'|A_t=a,S_t=s),\n",
    "$$\n",
    "for any $a$. As such, \n",
    "\\begin{eqnarray*}\n",
    "\\nu(s) &\\ge& \\sum_a \\pi_0(a|s) \\left[r(s, a) + \\gamma \\sum_{s'} \\nu(s') \\P(S_{t+1}=s'|A_t=a,S_t=s)\\right]\\\\\n",
    "&=&\\sum_a \\pi_0(a|s) r(s, a) + \\gamma \\sum_{a,s'} \\pi_0(a|s) \\nu(s') \\P(S_{t+1}=s'|A_t=a,S_t=s)\n",
    "\\end{eqnarray*}\n",
    "\n",
    "The first term on the right-hand-side corresponds to the immediate reward under $\\pi_0$. Iteratively applying the above and choose $\\pi_k$ as the weight function at each iteration we get \n",
    "\n",
    "$$\n",
    "\\nu(s) \\ge \\sum_{a,s}\\pi_0(a|s)r(s, a) + \\sum_{k=1}^K \\gamma^k \\E^{\\pi}(r(S_k, A_k) \\mid S_0 = s) + \\gamma^{K+1} \\E^{\\pi}(\\nu(S_{K+1}) \\mid s_0 = s) \n",
    "$$\n",
    "\n",
    "As $\\nu$ is bounded, the last term converges to zero as $K \\to \\infty$. This leads to\n",
    "$$\\nu(s) \\ge \\Vpi(s)$$\n",
    "\n",
    "<!--#### Proof of $\\le$\n",
    "\n",
    "Also by definition of $\\nu$ we have that\n",
    "\n",
    "$$\n",
    "\\inf_{\\pi \\in \\mathrm{SD}} \\sup_{s \\in \\S} \\big[\\nu(s) - (r(\\pi(s), s) + \\gamma \\sum_{s'} \\nu(s') \\P(s'|s, \\pi(s)))\\big] \\le 0.\n",
    "$$\n",
    "\n",
    "Therefore, for any $\\epsilon > 0$ there exists a policy $\\pi_\\epsilon \\in \\mathrm{SD}$ such that \n",
    "\n",
    "$$\n",
    "\\sup_{s} \\big[\\nu(s) - (r(\\pi_\\epsilon(s), S) + \\gamma \\sum_{s'} \\nu(s') \\P(s'| \\pi_\\epsilon(s), s))\\big] \\le \\epsilon. \\qquad\\qquad\\qquad \\text{(1)}\n",
    "$$\n",
    "\n",
    "Define the transition matrix $L_\\epsilon \\colon \\R^\\S \\to \\R^\\S$ such that the entries fulfill $(L_\\epsilon)_{s, s'} = \\P(s' | \\pi_{\\epsilon}(s), s) $ for all $s, s'$. -->\n",
    "\n",
    "<!-- $$\n",
    "\\mathcal \\L(\\nu(s)e_s) = \\sum_{s' \\in \\S} \\nu(s') \\P(s'|s, \\pi(s)) \\qquad \\forall S \\in \\S\n",
    "$$\n",
    " -->\n",
    "<!--Crucially, the largest eigenvalue of $L_\\epsilon$ is 1, i.e. $\\|L_\\epsilon \\|_2 = 1$.\n",
    "\n",
    "This implies that $I - \\gamma L_\\epsilon$ is invertible, in particular its inverse is given by \n",
    "$$(I - \\gamma L_\\epsilon)^{-1} = \\sum_{k = 0}^\\infty \\gamma^k L_\\epsilon^k.$$\n",
    "\n",
    "In slight abuse of notation we let $\\nu \\in \\R^{\\S}$ be the vector whose components are $\\nu(s)$ for $s \\in \\S$ and we let $r_\\epsilon \\in \\R^\\S$ be the vector whose components are $r(\\pi_\\epsilon(s), s)$ for $s \\in \\S$.\n",
    "\n",
    "By assumption and reordering the terms in Inequality (1) we then have that \n",
    "\n",
    "$$(I - \\gamma L_\\epsilon)\\nu \\le  \\epsilon 1_\\S + r_\\epsilon,$$\n",
    "\n",
    "where $1_\\S$ is the all-ones vector.\n",
    "\n",
    "Note that the matrix $\\sum_{k = 0}^\\infty \\gamma^k L_\\epsilon^k$ has no negative entries. Because of this we can multiply this matrix to the above inequality and get \n",
    "\n",
    "$$\\nu = \\sum_{k = 0}^\\infty \\gamma^k L_\\epsilon^k (I - \\gamma L_\\epsilon)\\nu \\le  \\sum_{k = 0}^\\infty \\gamma^k L_\\epsilon^k(\\epsilon 1_\\S + r_\\epsilon).$$\n",
    "\n",
    "Recall the definition of $$V^{\\pi_\\epsilon}(s)$$ In our notation we can express it as \n",
    "$$V(\\pi_\\epsilon, s) = \\sum_{k = 0}^\\infty \\gamma^k L_\\epsilon^k r_\\epsilon.$$\n",
    "Further note that $\\|\\sum_{k = 0}^\\infty \\gamma^k\\|_2 \\le 1/(1-\\gamma)$. Hence we have that \n",
    "\n",
    "$$\\nu \\le V + \\epsilon \\cdot \\frac{1}{1-\\gamma} 1_{\\S},$$\n",
    "\n",
    "where again in abuse of notation we interpret $V$ as a vector.\n",
    "\n",
    "Taking the limit $\\epsilon \\to 0$ we learn that $\\nu \\le V$, finishing Step 2 of the Proof.-->\n",
    "\n",
    "### Proof of 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "controlled-young",
   "metadata": {},
   "source": [
    "We will use [Banach's fixed point theorem](https://en.wikipedia.org/wiki/Banach_fixed-point_theorem) to show the existence of a funciton $\\nu_0$ that satisfies the optimal bellman equation. Suppose the existence has been proven. It follows from Bellman's equation that\n",
    "$$\n",
    "\\nu_0(s) = \\max_{a} \\left[r(s, a) + \\gamma \\sum_{s'} \\nu_0(s') \\P(S_{t+1}=s'|A_t=a,S_t=s)\\right]\n",
    "$$\n",
    "\n",
    "For each state $s$, there exists an action that maximizes the right-hand-side. This allows us to define a stationary deterministic policy $\\pi^*$ that takes $s$ as input and outputs the argmax. The Bellman optimality equation can be rewritten as \n",
    "$$\n",
    "\\nu_0(s) = \\left[r(s, \\pi^*(s)) + \\gamma \\sum_{s'} \\nu_0(s') \\P(S_{t+1}=s'|A_t=\\pi^*(s),S_t=s)\\right].\n",
    "$$\n",
    "Using similar arguments in Proof of 2., we have by iteratively applying the above equation that\n",
    "$$\n",
    "\\nu_0(s) = \\sum_{s}r(s, \\pi^*(s)) + \\sum_{k=1}^K \\gamma^k \\E^{\\pi^*}(r(S_k, A_k) \\mid S_0 = s) + \\gamma^{K+1} \\E^{\\pi^*}(\\nu_0(S_{K+1}) \\mid s_0 = s) \n",
    "$$\n",
    "Letting $K\\to \\infty$ leads to $\\nu_0(s)=V^{\\pi^*}(s)$ for any $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-musical",
   "metadata": {},
   "source": [
    "It remains to show the existence. Consider the following function $F$ that maps a function $\\nu$ to another function $F_{\\nu}$ defined on the state space such that\n",
    "\n",
    "$$F_{\\nu}(s) = \\max_{a} \\left[r(s,a) + \\gamma \\sum_{s'} \\nu(s') \\P(S_{t+1}=s'|A_t=a,S_t=s)\\right].$$\n",
    "\n",
    "Then for two different functions $\\nu_1, \\nu_2$ we have \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\max_s |F_{\\nu_1}(s) - F_{\\nu_2}(s)| &= \\max_s \\left|\\max_{a} \\left[r(s,a) + \\gamma \\sum_{s'} \\nu_1(s') \\P(S_{t+1}=s'|A_t=a,S_t=s)\\right]-\\max_{a} \\left[r(s,a) + \\gamma \\sum_{s'} \\nu_2(s') \\P(S_{t+1}=s'|A_t=a,S_t=s)\\right]\\right| \\\\\n",
    "& \\le \\max_s \\max_a \\left|\\left[r(s,a) + \\gamma \\sum_{s'} \\nu_1(s') \\P(S_{t+1}=s'|A_t=a,S_t=s)\\right]-\\left[r(s,a) + \\gamma \\sum_{s'} \\nu_2(s') \\P(S_{t+1}=s'|A_t=a,S_t=s)\\right]\\right|\\\\\n",
    "& \\le \\gamma \\max_{a,s} \\left|\\sum_{s'} [\\nu_1(s')-\\nu_2(s')]\\P(S_{t+1}=s'|A_t=a,S_t=s) \\right| \\\\\n",
    "&\\le \\gamma \\max_{a,s,s'} |\\nu_1(s')-\\nu_2(s')| \\left[\\sum_{s'} |\\P(S_{t+1}=s'|A_t=a,S_t=s)|\\right]\\\\\n",
    "& = \\gamma \\max_{a,s} |\\nu_1(s)-\\nu_2(s)|\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Hence by [Banach's fixed point theorem](https://en.wikipedia.org/wiki/Banach_fixed-point_theorem) we have a unique $\\nu^*$ such that $F(\\nu^*) = \\nu^*$. The proof is hence completed."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
