{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "684dd2a1",
   "metadata": {},
   "source": [
    "# Seminar 4 Part 2: Advantage Actor-Critic\n",
    "-----------------\n",
    "\n",
    "Actor-Critic methods are policy gradient methods that represent the policy function independent of the value function. \n",
    "\n",
    "A policy function (or policy) returns a probability distribution over actions that the agent can take based on the given state.\n",
    "A value function determines the expected return for an agent starting at a given state and acting according to a particular policy forever after.\n",
    "\n",
    "In the Actor-Critic method, the policy is referred to as the ***actor*** that proposes a set of possible actions given a state, and the estimated value function is referred to as the ***critic***, which evaluates actions taken by the ***actor*** based on the given policy.\n",
    "\n",
    "In this tutorial, both the ***Actor*** and ***Critic*** will be represented using neural networks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502381ca",
   "metadata": {},
   "source": [
    "## CartPole-v1\n",
    "\n",
    "In the [CartPole-v1 environment](https://gym.openai.com/envs/CartPole-v1), a pole is attached to a cart moving along a frictionless track. \n",
    "The pole starts upright and the goal of the agent is to prevent it from falling over by applying a force of -1 or +1 to the cart. \n",
    "A reward of +1 is given for every time step the pole remains upright.\n",
    "An episode ends when (1) the pole is more than 15 degrees from vertical or (2) the cart moves more than 2.4 units from the center.\n",
    "\n",
    "<center>\n",
    "  <figure>\n",
    "    <img src=\"./graphs/cartpole-v0.gif\">\n",
    "    <figcaption>\n",
    "      Trained actor-critic model in Cartpole-v1 environment\n",
    "    </figcaption>\n",
    "  </figure>\n",
    "</center>\n",
    "\n",
    "The problem is considered \"solved\" when the average total reward for the episode reaches 495 over 100 consecutive trials."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169aebdd",
   "metadata": {},
   "source": [
    "The code below is organized upon this [repository](https://github.com/yc930401/Actor-Critic-pytorch)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d663710",
   "metadata": {},
   "source": [
    "## Setup Environment\n",
    "\n",
    "Import necessary packages and configure global settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "244d12b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of state:  4\n",
      "Number of action:  2\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "\n",
    "env = gym.make(\"CartPole-v1\").unwrapped\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "\n",
    "print(\"Dimension of state: \", state_size)\n",
    "print(\"Number of action: \", action_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12813009",
   "metadata": {},
   "source": [
    "For Cartpole-v1, there are four values representing the state: \n",
    "\n",
    "- cart position, \n",
    "- cart-velocity, \n",
    "- pole angle, \n",
    "- pole velocity. \n",
    "\n",
    "The agent can take two actions to push the cart: \n",
    "- left (0),\n",
    "- right (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9717fc",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "The *Actor* and *Critic* will be modeled using neural networks that generates the action probabilities and critic value, respectively.  \n",
    "\n",
    "During the forward pass, the model will take in the state as the input and will output both action probabilities and critic value $V$, which models the state-dependent [value function](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#value-functions). The goal is to train a model that chooses actions based on a policy $\\pi$ that maximizes expected [return](https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#reward-and-return)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "30320e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "class Actor(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Actor, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = nn.Linear(128, 256)\n",
    "        self.linear3 = nn.Linear(256, self.action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        output = self.linear3(output)\n",
    "        distribution = Categorical(F.softmax(output, dim=-1))\n",
    "        return distribution\n",
    "\n",
    "\n",
    "class Critic(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(Critic, self).__init__()\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.linear1 = nn.Linear(self.state_size, 128)\n",
    "        self.linear2 = nn.Linear(128, 256)\n",
    "        self.linear3 = nn.Linear(256, 1)\n",
    "\n",
    "    def forward(self, state):\n",
    "        output = F.relu(self.linear1(state))\n",
    "        output = F.relu(self.linear2(output))\n",
    "        value = self.linear3(output)\n",
    "        return value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a37422",
   "metadata": {},
   "source": [
    "### Loss functions for actor and critic\n",
    "\n",
    "#### Actor loss\n",
    "\n",
    "The actor loss is based on [policy gradients with the critic as a state dependent baseline](https://www.youtube.com/watch?v=EKqxumCuAAY&t=62m23s) and computed with single-sample (per-episode) estimates.\n",
    "\n",
    "$$L_{\\text{actor}} = -\\sum^{T}_{t=1} \\log\\pi_{\\theta}(a_{t} | s_{t})[G(s_{t}, a_{t})  - V^{\\pi}_{\\theta}(s_{t})]$$\n",
    "\n",
    "where:\n",
    "- $T$: the number of timesteps per episode, which can vary per episode\n",
    "- $s_{t}$: the state at timestep $t$\n",
    "- $a_{t}$: chosen action at timestep $t$ given state $s$\n",
    "- $\\pi_{\\theta}$: is the policy (***actor***) parameterized by $\\theta$\n",
    "- $V^{\\pi}_{\\theta}$: is the value function (***critic***) also parameterized by $\\theta$\n",
    "- $G(s_{t}, a_{t})$: the expected return for a given state, action pair at timestep $t$\n",
    "- $G(s_{t}, a_{t})  - V^{\\pi}_{\\theta}(s_{t})$: the advantage that indicates the goodness of an action given a particular state over a random action selected according to the policy $\\pi$ for that state.\n",
    "    - Using the advantage may result in high variance during training. \n",
    "\n",
    "A negative term is added to the sum since the idea is to maximize the probabilities of actions yielding higher rewards by minimizing the combined loss.\n",
    "\n",
    "##### Empirical merit of using the advantage\n",
    "\n",
    "\n",
    "Without the advantage, the algorithm would try to increase probabilities for actions taken on a particular state based on expected return $G_t$, which may not make much of a difference if the relative probabilities between actions remain the same.\n",
    "\n",
    "For instance, suppose that two actions for a given state would yield the same expected return. Without the advantage, the algorithm would try to raise the probability of these actions based on the objective $J$. With the advantage, it may turn out that there's no advantage ($G - V = 0$) and thus no benefit gained in increasing the actions' probabilities and the algorithm would set the gradients to zero.\n",
    "\n",
    "#### Critic loss\n",
    "\n",
    "Training $V$ to be as close possible to $G$ can be set up as a regression problem with the following square loss function:\n",
    "\n",
    "$$L_{\\text{critic}} = \\sum_{t=1}^T(G(s_t, a_t) - V^{\\pi}_{\\theta}(s_t))^2.$$\n",
    "\n",
    "##### Computing expected returns $G(s_t, a_t)$\n",
    "\n",
    "The sequence of rewards $\\{r_{1}, r_2, \\ldots, r_t, \\ldots, r_T\\}$ collected during one episode is converted into a sequence of expected returns $\\{G_{1}, G_2, \\ldots, G_t, \\ldots, G_T\\}$ where \n",
    "\n",
    "$$G_{t} = \\sum^{T}_{t'=t} \\gamma^{t'-t}r_{t'}$$\n",
    "\n",
    "and $\\gamma\\in (0, 1]$. The implementation is below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f76fac91",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns(rewards, masks, gamma=0.99):\n",
    "    R = torch.zeros(1)\n",
    "    returns = []\n",
    "    for step in reversed(range(len(rewards))):\n",
    "        R = rewards[step] + gamma * R * masks[step]\n",
    "        returns.insert(0, R)\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c03485c",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "To train the agent, you will follow these steps:\n",
    "\n",
    "1. Run the agent on the environment to collect training data per episode.\n",
    "2. Compute expected return at each time step.\n",
    "3. Compute the loss for the actor and critic models.\n",
    "4. Compute gradients and update network parameters.\n",
    "5. Repeat 1-4 until either success criterion or max episodes has been reached.\n",
    "\n",
    "Note that this method differs algorithmically from the lecture and also from the Sutton-Barto Book. The weights are updated online after each step, while the implementation in this Notebook only updates parameters in an offline manner at the end of an episode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4fcfa32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import count\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def trainIters(actor, critic, n_iters):\n",
    "    optimizerA = optim.Adam(actor.parameters())\n",
    "    optimizerC = optim.Adam(critic.parameters())\n",
    "    for iter in range(n_iters):\n",
    "        state = env.reset(seed=iter)\n",
    "        state = state[0]\n",
    "        log_probs = []\n",
    "        values = []\n",
    "        rewards = []\n",
    "        masks = []\n",
    "        env.reset()\n",
    "\n",
    "        for i in count():\n",
    "            env.render()\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = actor(state), critic(state)\n",
    "\n",
    "            action = dist.sample()\n",
    "            next_state, reward, done, _, _ = env.step(action.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action).unsqueeze(0)\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.tensor([reward], dtype=torch.float, device=device))\n",
    "            masks.append(torch.tensor([1 - done], dtype=torch.float, device=device))\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                print(\"Iteration: {}, Score: {}\".format(iter, i))\n",
    "                break\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        returns = compute_returns(rewards, masks)\n",
    "\n",
    "        log_probs = torch.cat(log_probs)\n",
    "        returns = torch.cat(returns).detach()\n",
    "        values = torch.cat(values)\n",
    "\n",
    "        advantage = returns - values\n",
    "\n",
    "        actor_loss = -(log_probs * advantage.detach()).mean()\n",
    "        critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "        optimizerA.zero_grad()\n",
    "        optimizerC.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        critic_loss.backward()\n",
    "        optimizerA.step()\n",
    "        optimizerC.step()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbc7b794",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 0, Score: 14\n",
      "Iteration: 1, Score: 14\n",
      "Iteration: 2, Score: 36\n",
      "Iteration: 3, Score: 11\n",
      "Iteration: 4, Score: 24\n",
      "Iteration: 5, Score: 12\n",
      "Iteration: 6, Score: 28\n",
      "Iteration: 7, Score: 21\n",
      "Iteration: 8, Score: 17\n",
      "Iteration: 9, Score: 21\n",
      "Iteration: 10, Score: 22\n",
      "Iteration: 11, Score: 16\n",
      "Iteration: 12, Score: 27\n",
      "Iteration: 13, Score: 34\n",
      "Iteration: 14, Score: 24\n",
      "Iteration: 15, Score: 20\n",
      "Iteration: 16, Score: 29\n",
      "Iteration: 17, Score: 21\n",
      "Iteration: 18, Score: 11\n",
      "Iteration: 19, Score: 50\n",
      "Iteration: 20, Score: 23\n",
      "Iteration: 21, Score: 26\n",
      "Iteration: 22, Score: 30\n",
      "Iteration: 23, Score: 38\n",
      "Iteration: 24, Score: 38\n",
      "Iteration: 25, Score: 97\n",
      "Iteration: 26, Score: 35\n",
      "Iteration: 27, Score: 20\n",
      "Iteration: 28, Score: 41\n",
      "Iteration: 29, Score: 44\n",
      "Iteration: 30, Score: 21\n",
      "Iteration: 31, Score: 34\n",
      "Iteration: 32, Score: 21\n",
      "Iteration: 33, Score: 34\n",
      "Iteration: 34, Score: 41\n",
      "Iteration: 35, Score: 81\n",
      "Iteration: 36, Score: 34\n",
      "Iteration: 37, Score: 21\n",
      "Iteration: 38, Score: 8\n",
      "Iteration: 39, Score: 49\n",
      "Iteration: 40, Score: 14\n",
      "Iteration: 41, Score: 26\n",
      "Iteration: 42, Score: 38\n",
      "Iteration: 43, Score: 23\n",
      "Iteration: 44, Score: 30\n",
      "Iteration: 45, Score: 51\n",
      "Iteration: 46, Score: 48\n",
      "Iteration: 47, Score: 44\n",
      "Iteration: 48, Score: 56\n",
      "Iteration: 49, Score: 48\n",
      "Iteration: 50, Score: 44\n",
      "Iteration: 51, Score: 50\n",
      "Iteration: 52, Score: 57\n",
      "Iteration: 53, Score: 24\n",
      "Iteration: 54, Score: 40\n",
      "Iteration: 55, Score: 27\n",
      "Iteration: 56, Score: 83\n",
      "Iteration: 57, Score: 54\n",
      "Iteration: 58, Score: 42\n",
      "Iteration: 59, Score: 50\n",
      "Iteration: 60, Score: 34\n",
      "Iteration: 61, Score: 18\n",
      "Iteration: 62, Score: 47\n",
      "Iteration: 63, Score: 88\n",
      "Iteration: 64, Score: 112\n",
      "Iteration: 65, Score: 41\n",
      "Iteration: 66, Score: 53\n",
      "Iteration: 67, Score: 42\n",
      "Iteration: 68, Score: 38\n",
      "Iteration: 69, Score: 96\n",
      "Iteration: 70, Score: 27\n",
      "Iteration: 71, Score: 26\n",
      "Iteration: 72, Score: 27\n",
      "Iteration: 73, Score: 65\n",
      "Iteration: 74, Score: 60\n",
      "Iteration: 75, Score: 86\n",
      "Iteration: 76, Score: 82\n",
      "Iteration: 77, Score: 49\n",
      "Iteration: 78, Score: 105\n",
      "Iteration: 79, Score: 102\n",
      "Iteration: 80, Score: 122\n",
      "Iteration: 81, Score: 100\n",
      "Iteration: 82, Score: 68\n",
      "Iteration: 83, Score: 62\n",
      "Iteration: 84, Score: 56\n",
      "Iteration: 85, Score: 112\n",
      "Iteration: 86, Score: 27\n",
      "Iteration: 87, Score: 65\n",
      "Iteration: 88, Score: 117\n",
      "Iteration: 89, Score: 115\n",
      "Iteration: 90, Score: 119\n",
      "Iteration: 91, Score: 177\n",
      "Iteration: 92, Score: 104\n",
      "Iteration: 93, Score: 83\n",
      "Iteration: 94, Score: 87\n",
      "Iteration: 95, Score: 229\n",
      "Iteration: 96, Score: 260\n",
      "Iteration: 97, Score: 158\n",
      "Iteration: 98, Score: 111\n",
      "Iteration: 99, Score: 234\n",
      "Iteration: 100, Score: 209\n",
      "Iteration: 101, Score: 122\n",
      "Iteration: 102, Score: 133\n",
      "Iteration: 103, Score: 167\n",
      "Iteration: 104, Score: 133\n",
      "Iteration: 105, Score: 53\n",
      "Iteration: 106, Score: 64\n",
      "Iteration: 107, Score: 113\n",
      "Iteration: 108, Score: 65\n",
      "Iteration: 109, Score: 175\n",
      "Iteration: 110, Score: 153\n",
      "Iteration: 111, Score: 185\n",
      "Iteration: 112, Score: 390\n",
      "Iteration: 113, Score: 151\n",
      "Iteration: 114, Score: 175\n",
      "Iteration: 115, Score: 144\n",
      "Iteration: 116, Score: 172\n",
      "Iteration: 117, Score: 239\n",
      "Iteration: 118, Score: 330\n",
      "Iteration: 119, Score: 139\n",
      "Iteration: 120, Score: 208\n",
      "Iteration: 121, Score: 183\n",
      "Iteration: 122, Score: 249\n",
      "Iteration: 123, Score: 140\n",
      "Iteration: 124, Score: 194\n",
      "Iteration: 125, Score: 159\n",
      "Iteration: 126, Score: 269\n",
      "Iteration: 127, Score: 181\n",
      "Iteration: 128, Score: 167\n",
      "Iteration: 129, Score: 172\n",
      "Iteration: 130, Score: 230\n",
      "Iteration: 131, Score: 146\n",
      "Iteration: 132, Score: 296\n",
      "Iteration: 133, Score: 141\n",
      "Iteration: 134, Score: 102\n",
      "Iteration: 135, Score: 64\n",
      "Iteration: 136, Score: 339\n",
      "Iteration: 137, Score: 128\n",
      "Iteration: 138, Score: 149\n",
      "Iteration: 139, Score: 120\n",
      "Iteration: 140, Score: 105\n",
      "Iteration: 141, Score: 94\n",
      "Iteration: 142, Score: 148\n",
      "Iteration: 143, Score: 176\n",
      "Iteration: 144, Score: 176\n",
      "Iteration: 145, Score: 169\n",
      "Iteration: 146, Score: 117\n",
      "Iteration: 147, Score: 316\n",
      "Iteration: 148, Score: 839\n",
      "Iteration: 149, Score: 323\n",
      "Iteration: 150, Score: 359\n",
      "Iteration: 151, Score: 361\n",
      "Iteration: 152, Score: 278\n",
      "Iteration: 153, Score: 238\n",
      "Iteration: 154, Score: 231\n",
      "Iteration: 155, Score: 251\n",
      "Iteration: 156, Score: 221\n",
      "Iteration: 157, Score: 243\n",
      "Iteration: 158, Score: 199\n",
      "Iteration: 159, Score: 109\n",
      "Iteration: 160, Score: 148\n",
      "Iteration: 161, Score: 118\n",
      "Iteration: 162, Score: 226\n",
      "Iteration: 163, Score: 181\n",
      "Iteration: 164, Score: 154\n",
      "Iteration: 165, Score: 166\n",
      "Iteration: 166, Score: 213\n",
      "Iteration: 167, Score: 229\n",
      "Iteration: 168, Score: 241\n",
      "Iteration: 169, Score: 212\n",
      "Iteration: 170, Score: 233\n",
      "Iteration: 171, Score: 253\n",
      "Iteration: 172, Score: 263\n",
      "Iteration: 173, Score: 179\n",
      "Iteration: 174, Score: 338\n",
      "Iteration: 175, Score: 327\n",
      "Iteration: 176, Score: 350\n",
      "Iteration: 177, Score: 88\n",
      "Iteration: 178, Score: 213\n",
      "Iteration: 179, Score: 440\n",
      "Iteration: 180, Score: 406\n",
      "Iteration: 181, Score: 174\n",
      "Iteration: 182, Score: 200\n",
      "Iteration: 183, Score: 271\n",
      "Iteration: 184, Score: 176\n",
      "Iteration: 185, Score: 170\n",
      "Iteration: 186, Score: 72\n",
      "Iteration: 187, Score: 200\n",
      "Iteration: 188, Score: 182\n",
      "Iteration: 189, Score: 303\n",
      "Iteration: 190, Score: 182\n",
      "Iteration: 191, Score: 183\n",
      "Iteration: 192, Score: 180\n",
      "Iteration: 193, Score: 445\n",
      "Iteration: 194, Score: 272\n",
      "Iteration: 195, Score: 125\n",
      "Iteration: 196, Score: 643\n",
      "Iteration: 197, Score: 318\n",
      "Iteration: 198, Score: 301\n",
      "Iteration: 199, Score: 120\n"
     ]
    }
   ],
   "source": [
    "actor = Actor(state_size, action_size).to(device)\n",
    "critic = Critic(state_size, action_size).to(device)\n",
    "trainIters(actor, critic, n_iters=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b2d2a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
